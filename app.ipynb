{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de16570-6897-4687-84d8-9b42c2e369fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-gemini\n",
      "  Downloading llama_index_llms_gemini-0.3.7-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.2 (from llama-index-llms-gemini)\n",
      "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-llms-gemini) (0.11.21)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-llms-gemini) (10.3.0)\n",
      "Collecting google-ai-generativelanguage==0.6.4 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_ai_generativelanguage-0.6.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_api_core-2.22.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting google-api-python-client (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_api_python_client-2.151.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (5.28.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dines\\anaconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.14.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.9.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\dines\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dines\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2023.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dines\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.23.0)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\dines\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dines\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dines\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.14.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (1.67.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (3.0.9)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (23.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.4.8)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading llama_index_llms_gemini-0.3.7-py3-none-any.whl (6.1 kB)\n",
      "Downloading google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.7 kB ? eta -:--:--\n",
      "   -------------------------------------- - 143.4/150.7 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.7/150.7 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
      "   ---------------------------------------- 0.0/679.1 kB ? eta -:--:--\n",
      "   ------------------------- ------------- 440.3/679.1 kB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 679.1/679.1 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.22.0-py3-none-any.whl (156 kB)\n",
      "   ---------------------------------------- 0.0/156.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 156.5/156.5 kB ? eta 0:00:00\n",
      "Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading google_api_python_client-2.151.0-py2.py3-none-any.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.6/12.5 MB 20.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.2/12.5 MB 15.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.7/12.5 MB 15.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.1/12.5 MB 14.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.8/12.5 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.5/12.5 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.0/12.5 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.3/12.5 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.1/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.7/12.5 MB 13.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.4/12.5 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.0/12.5 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.1/12.5 MB 14.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.7/12.5 MB 14.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.3/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.6/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.3/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.0/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.5/12.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 13.6 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.9/96.9 kB ? eta 0:00:00\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, llama-index-llms-gemini\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "Successfully installed google-ai-generativelanguage-0.6.4 google-api-core-2.22.0 google-api-python-client-2.151.0 google-auth-2.35.0 google-auth-httplib2-0.2.0 google-generativeai-0.5.4 googleapis-common-protos-1.65.0 grpcio-status-1.62.3 httplib2-0.22.0 llama-index-llms-gemini-0.3.7 proto-plus-1.25.0 protobuf-4.25.5 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.67.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf9bf06-a08c-4bae-a006-e497d7c007c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\dines\\anaconda3\\lib\\site-packages (1.20.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\dines\\anaconda3\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea074d8-92cb-4827-b594-7c511b8ec9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(\"Dataset\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1921bee5-1768-4705-a42e-9e0b3913c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser.text import SentenceSplitter\n",
    "# Initialize the SentenceSplitter with a specific chunk size\n",
    "text_parser = SentenceSplitter(chunk_size=1024)\n",
    "text_chunks = [] # This will hold all the chunks of text from all documents\n",
    "doc_idxs = [] # This will keep track of the document each chunk came from\n",
    "for doc_idx, doc in enumerate(docs):\n",
    " # Split the current document's text into chunks\n",
    " cur_text_chunks = text_parser.split_text(doc.text)\n",
    " \n",
    " # Extend the list of all text chunks with the chunks from the current document\n",
    " text_chunks.extend(cur_text_chunks)\n",
    " \n",
    " # Extend the document index list with the index of the current document, repeated for each chunk\n",
    " doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69fd95a-b96f-4ba8-9a3b-f75a81ce7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "nodes = [] # This will hold all TextNode objects created from the text chunks\n",
    "# Iterate over each text chunk and its index\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    " # Create a TextNode object with the current text chunk\n",
    " node = TextNode(text=text_chunk)\n",
    " \n",
    " # Retrieve the source document using the current index mapped through doc_idxs\n",
    " src_doc = docs[doc_idxs[idx]]\n",
    " \n",
    " # Assign the source document's metadata to the node's metadata attribute\n",
    " node.metadata = src_doc.metadata\n",
    " \n",
    " # Append the newly created node to the list of nodes\n",
    " nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad18fca4-13c5-4c43-9a4f-403593a2ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "import qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5806addf-b451-482b-a8ac-f5ec1c1bd176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"financialnews\")\n",
    "#from qdrant_client import QdrantClient\n",
    "#client = QdrantClient(host=\"localhost\", port=6333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=client, collection_name=\"collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36025c28-efd4-454c-b65e-4e0e34d2a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_API_KEY=\"your-api-key\"\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_API_KEY = \"your-api-key\"\n",
    "import os\n",
    "GOOGLE_API_KEY = \"api_key\" # add your GOOGLE API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cea573-ce46-4e88-bf4e-6c5246138f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f0cc33f-9197-4f0d-bdec-5e528f3c741e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca69307bb5f4e38a08be0b5406e0f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ffeb29b2f6443b89483380c3825650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dines\\AppData\\Local\\Temp\\fastembed_cache\\models--qdrant--all-MiniLM-L6-v2-onnx. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d05a31ea6c4c76839f80b81db5b434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7554618efe7240788d5347104ecd42e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9dafbe40ac5438f95347c88e396247d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bc5abc3bb74447a85092ad2b759e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "#embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embed_model = FastEmbedEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9650a1eb-0dd0-49a9-a5a6-ee8b8c55c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\dines\\anaconda3\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dines\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\dines\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dines\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\dines\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\dines\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\dines\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\dines\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3659f27-622b-4f36-9150-d45b625bf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for node in nodes:\n",
    " node_embedding = embed_model.get_text_embedding(\n",
    " node.get_content(metadata_mode=\"all\")\n",
    " )\n",
    " node.embedding = node_embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb027b4-4726-4f67-ab8c-9a5907dbcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = Gemini(model=\"models/gemini-pro\")\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=1024)]\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec8c7d3-fec7-40e6-8f54-e5823d03eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = VectorStoreIndex(\n",
    " nodes=nodes,\n",
    " storage_context=storage_context,\n",
    "transformations=Settings.transformations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60698e3-92cb-4a32-9dfb-d9badef0c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a81ebc2f-dd34-42aa-b13b-4c9971c731da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    " retriever=vector_retriever,\n",
    " response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0deaafe4-a7b5-45c9-b5eb-03729add9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(vector_query_engine, hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109c8ef1-45cc-4aee-ba0f-4e3e872578de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries(query_str):\n",
    " response = hyde_query_engine.query(query_str)\n",
    " return str(response)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0e3cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "gr.close_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "931aa38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Glass()) as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Welcome to Gemini-Powered News Chatbot!\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = queries(message)\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90ab490-7896-4165-87b1-51db66260f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 2018, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_46084\\1032989736.py\", line 10, in respond\n",
      "    bot_message = queries(message)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\AppData\\Local\\Temp\\ipykernel_46084\\1769204422.py\", line 2, in queries\n",
      "    response = hyde_query_engine.query(query_str)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 311, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py\", line 52, in query\n",
      "    query_result = self._query(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 311, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\transform_query_engine.py\", line 83, in _query\n",
      "    query_bundle = self._query_transform.run(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\query\\query_transform\\base.py\", line 73, in run\n",
      "    return self._run(query_bundle, metadata=metadata)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 311, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\query\\query_transform\\base.py\", line 154, in _run\n",
      "    hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 311, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 596, in predict\n",
      "    chat_response = self.chat(messages)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 311, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py\", line 173, in wrapped_llm_chat\n",
      "    f_return_val = f(_self, messages, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\llama_index\\llms\\gemini\\base.py\", line 204, in chat\n",
      "    response = chat.send_message(next_msg)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 505, in send_message\n",
      "    self._check_response(response=response, stream=stream)\n",
      "  File \"C:\\Users\\dines\\anaconda3\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 532, in _check_response\n",
      "    raise generation_types.StopCandidateException(response.candidates[0])\n",
      "google.generativeai.types.generation_types.StopCandidateException: index: 0\n",
      "finish_reason: RECITATION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26bb37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5938f3-bbec-4e86-b681-12c214f564cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' important modules to install----\n",
    "pip install llama-index\n",
    "pip install llama-index-embeddings-fastembed\n",
    "pip install --upgrade llama-index\n",
    "pip install qdrant-client\n",
    "pip install git+https://github.com/jerryjliu/llama_index.git\n",
    "pip install llama-index-vector-stores-chroma\n",
    "pip install llama-index\n",
    "pip install llama-index-vector-stores-qdrant\n",
    "pip install llama-index-sparse-embeddings-fastembed \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
